---
confluent_services:
  - kafka
  - kafka-rest
  - zookeeper

kafka_ip: "{{ ansible_all_ipv4_addresses | first }}"
kafka_plaintext_port: 9092
kafka_security_port: 9093
kafka_group: >-
  {%- if groups['kafka'] is defined -%}
  {{ groups['kafka'] }}
  {%- else -%}
  {{ ansible_play_hosts }}
  {%- endif -%}

kafka_system_users:
  enabled: yes
  group: wheel
  permissions:
    directory_mode: "0750"
    file_mode: "0640"
  include_to_sudoers: yes

dst_path: "/opt"
local_path: "/tmp"
confluent_version: "5.4.0"
scala_version: "2.12"
confluent_distribution: "confluent-community"
confluent_base: http://packages.confluent.io/archive
confluent_package_file: >-
  {{ confluent_version[:3] }}/{{ confluent_distribution }}
  -{{ confluent_version }}-{{ scala_version }}.tar.gz
confluent_url: "{{ confluent_base }}/{{ confluent_package_file | replace(' ', '') }}"

log_basepath: "/var/log"
data_basepath: "/var/data"
initscripts_path: "/usr/sbin"
conf_dest: "/etc/config"

# Oracle Java necessary vars. IF you are using ansiblebit.oracle-java
oracle_java_set_as_default: yes
oracle_java_version: 8

## Kafka Vars
reserved_broker_max: 10000
delete_topic_enable: "true"
num_network_threads: 3
num_io_threads: 8

socket_send_buffer_bytes: 102400
socket_receive_buffer_bytes: 102400
socket_request_max_bytes: 104857600

log_dir: "{{ log_basepath }}/kafka"
num_partitions: 1
num_recovery_threads_per_data_dir: 1
log_flush_interval_messages: 10000
log_flush_interval_ms: 1000
log_retention_hours: 6
log_retention_bytes: 1073741824
log_segment_bytes: 1073741824
log_retention_check_interval_ms: 300000
ion_timeout_ms: 6000

confluent_support_metrics_enable: yes
confluent_support_customer_id: "anonymous"
auto_create_topics: no
default_replication_factor: 1
offsets_replication_factor: "{{ [ kafka_group | length, 3 ] | min }}"
min_insync_replicas: 1

ssl:
  path: /var/private/ssl

local:
  files: ../files

kafka_file_descriptor_limit: 100000
kafka_env_file: "{{ confluent_kafka_env_file_path }}/kafka-env.sh"

kafka_init_memory_heapsize_with_units: 1G

kafka_max_memory_heapsize_with_units: >
  "{{ ((ansible_memtotal_mb/1024) | root | round | int | string ) + 'G' 
  if ((ansible_memtotal_mb/1024) | root | round | int)> 1 else '1G' }}"

## Kafka SSL Vars
kafka_security:
  server:
    keystore: "kafka.server.keystore.jks"
    truststore: "kafka.server.truststore.jks"
  client:
    truststore: "kafka.client.truststore.jks"

## Schema Registry Vars
listeners_schema: "http://0.0.0.0:8081"
kafkastore_topic: "schema-registry"
schema_registry_debug: "false"

## Zookeeper Vars
zookeeper_group: "{{ groups['zookeeper'] | default(ansible_play_hosts) }}"
zookeepers: >
  "{% for server in zookeeper_group -%}
  {{ hostvars.get(server).ansible_all_ipv4_addresses | first }}:{{ zookeeper_port }}
  {% endfor %}"
zookeeper_port: 2181
zookeeper_connect: "{{ zookeepers.split() | join(',') }}"
zookeeper_data_dir: "{{ data_basepath }}/zookeeper"
zookeeper_log_dir: "{{ data_basepath }}/logs/zookeeper"
zookeeper_max_cnxns: 0
zookeeper_init_limit: 5
zookeeper_sync_limit: 3
zookeeper_connection_timeout: 6000
zookeeper_file_descriptor_limit: 100000
zookeeper_env_file: "{{ confluent_kafka_env_file_path }}/zookeeper-env.sh"
zookeeper_init_memory_heapsize_with_units: 512M
zookeeper_max_memory_heapsize_with_units: >
  "{{ ((ansible_memtotal_mb/1024) | root | round | int | string ) + 'G' 
  if ((ansible_memtotal_mb/1024) | root | round | int)> 1 else '512M' }}"

# Zookeeper Garbage Collection Logging
zookeeper_gc_logger_enabled: true
zookeeper_gc_logger_name: "{{ log4j_log_dir }}/zookeeper-gc.log"
zookeeper_gc_logger_enable_opts: "-verbose:gc -Xloggc:{{ zookeeper_gc_logger_name }}"
zookeeper_gc_logger_number_files: 10
zookeeper_gc_logger_max_size: 10M
zookeeper_gc_logger_rotation_opts: >
  "-XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles={{ zookeeper_gc_logger_number_files }}
   -XX:GCLogFileSize={{ zookeeper_gc_logger_max_size }}"
zookeeper_gc_logger_format_opts: "-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
zookeeper_gc_logger_enabled_opts: > 
  "{{ zookeeper_gc_logger_enable_opts }} 
  {{ zookeeper_gc_logger_rotation_opts }} 
  {{ zookeeper_gc_logger_format_opts }}"
zookeeper_gc_logger_disabled_opts: "-Dnogclog"

## Services
confluent_kafka_env_file_path: "/etc/profile.d"

kafka_services:
  kafka:
    file_name: "kafka"
    user: "kafka"
    daemon_start: "{{ dst_path }}/confluent-{{ confluent_version }}/bin/kafka-server-start"
    daemon_stop: "{{ dst_path }}/confluent-{{ confluent_version }}/bin/kafka-server-stop"
    daemon_status: "ps ax | grep -i 'Kafka.properties' | grep java > /dev/null"
    daemon_opts: "{% if ansible_service_mgr == 'upstart' %}-daemon {% endif %}{{ conf_dest }}/kafka.properties"
    state: "started"
    enabled: yes
    limit_nofile: "{{ kafka_file_descriptor_limit }}"
    environment_file: "{{ kafka_env_file }}"

  kafka-rest:
    file_name: "kafka-rest"
    user: "kafka"
    daemon_start: "{{ dst_path }}/confluent-{{ confluent_version }}/bin/kafka-rest-start"
    daemon_stop: "{{ dst_path }}/confluent-{{ confluent_version }}/bin/kafka-rest-stop"
    daemon_status: "ps ax | grep -i 'kafka-rest' | grep java > /dev/null"
    daemon_opts: " -daemon"
    state: "started"
    enabled: yes

zookeeper_services:
  zookeeper:
    file_name: "zookeeper"
    user: "zookeeper"
    daemon_start: "{{ dst_path }}/confluent-{{ confluent_version }}/bin/zookeeper-server-start"
    daemon_stop: "{{ dst_path }}/confluent-{{ confluent_version }}/bin/zookeeper-server-stop"
    daemon_status: "ps ax | grep -i 'zookeeper.properties' | grep java > /dev/null"
    daemon_opts: >-
      {% if ansible_service_mgr in [ 'upstart', 'sysvinit' ] -%}
      -daemon
      {%- endif -%}
      {{ conf_dest }}/zookeeper.properties
    state: "started"
    enabled: yes
    limit_nofile: "{{ zookeeper_file_descriptor_limit }}"
    environment_file: "{{ zookeeper_env_file }}"

# default_kafka_topics:
## Create a test topic with retention.ms set
#  - name: test_topic
#    state: present # optional defaults to true
#    config:
#       - item: retention.ms
#         value: 3600000
## Create a 2nd test topic with defaults
#  - name: test_topic_2
## Create a 3rd test topic with compression.type and file.delete.delay.ms set
#  - name: test_topic_3
#    partitions: 24
#    replication_factor: 2
#    config:
#       - item: compression.type
#         value: zip
#       - item: file.delete.delay.ms
#         value: 30000
## Ensure our topic does no longer exist if it is in the topic list
#  - name: unused_topic
#    state: absent

# Log4J Vars
log4j_log_dir: "{{ dst_path }}/confluent-{{ confluent_version }}/logs"

log4j_root_logger_level: "INFO"

log4j_stdout_class: "org.apache.log4j.ConsoleAppender"

log4j_kafka_appender_class: "org.apache.log4j.RollingFileAppender"
log4j_kafka_appender_file: "{{ log4j_log_dir }}/server.log"
log4j_kafka_appender_max_file_size: "10MB"
log4j_kafka_appender_maxBackupIndex: "100"

log4j_state_change_appender_class: "org.apache.log4j.RollingFileAppender"
log4j_state_change_appender_file: "{{ log4j_log_dir }}/state-change.log"
log4j_state_change_appender_max_file_size: "10MB"
log4j_state_change_appender_max_backup_index: "100"

log4j_request_appender_class: "org.apache.log4j.RollingFileAppender"
log4j_request_appender_file: "{{ log4j_log_dir }}/kafka-request.log"
log4j_request_appender_max_file_size: "10MB"
log4j_request_appender_max_backup_index: "100"

log4j_cleaner_appender_class: "org.apache.log4j.RollingFileAppender"
log4j_cleaner_appender_file: "{{ log4j_log_dir }}/log-cleaner.log"
log4j_cleaner_appender_max_file_size: "10MB"
log4j_cleaner_appender_max_backup_index: "100"

log4j_controller_appender_class: "org.apache.log4j.RollingFileAppender"
log4j_controller_appender_file: "{{ log4j_log_dir }}/controller.log"
log4j_controller_appender_max_file_size: "10MB"
log4j_controller_appender_max_backup_index: "100"

log4j_authorizer_appender_class: "org.apache.log4j.RollingFileAppender"
log4j_authorizer_appender_file: "{{ log4j_log_dir }}/kafka-authorizer.log"
log4j_authorizer_appender_max_file_size: "10MB"
log4j_authorizer_appender_max_backup_index: "100"

log4j_logger_zookeeper_level: "INFO"
log4j_logger_kafka_server_log_level: "INFO"
log4j_logger_kafka_stdout_level: "INFO"
log4j_logger_kafka_request_logger_level: "WARN"
log4j_logger_kafka_network_request_channel_level: "WARN"
log4j_logger_kafka_controller_level: "TRACE"
log4j_logger_kafka_log_cleaner_level: "INFO"
log4j_logger_kafka_state_change_logger_level: "TRACE"
log4j_logger_kafka_authorizer_logger_level: "INFO"

log4j_additivity_kafka_request_logger: "false"
log4j_additivity_kafka_network_request_channel: "false"
log4j_additivity_kafka_controller: "false"
log4j_additivity_kafka_log_cleaner: "false"
log4j_additivity_kafka_state_change_logger: "false"
log4j_additivity_kafka_authorizer_logger: "false"

# Service Vars
timeoutStartSec: 90

# Enable it to create a link of log4j_log_dir in {{ log_basepath }}/confluent
create_link_log_dir: yes
